{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 38119,
     "status": "ok",
     "timestamp": 1743960059341,
     "user": {
      "displayName": "Patrick Chao",
      "userId": "07760527590390898325"
     },
     "user_tz": 240
    },
    "id": "PDXrIcrTAU6V",
    "outputId": "bf52bc15-9ed4-4bdf-cd1b-c2544ea1cd05"
   },
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2930005,
     "status": "ok",
     "timestamp": 1743962994527,
     "user": {
      "displayName": "Patrick Chao",
      "userId": "07760527590390898325"
     },
     "user_tz": 240
    },
    "id": "SMZAh2a8AMDm",
    "outputId": "d370c594-d7ae-4cfd-ccb3-89f627f2dd03"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.types as fot\n",
    "\n",
    "\n",
    "classes = [\"Dog\", \"Cat\", \"Deer\", \"Bear\", \"Bird\", \"Person\", \"Car\", \"Truck\", \"Airplane\"]\n",
    "\n",
    "base_export_dir = \"/content/dataset\"\n",
    "\n",
    "if not os.path.exists(base_export_dir):\n",
    "    os.makedirs(base_export_dir)\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"Downloading Open Images subset for class '{cls}'...\")\n",
    "\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        \"open-images-v6\",\n",
    "        split=\"train\",\n",
    "        classes=[cls],\n",
    "        max_samples=500,  # try with fewer samples for testing\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    export_dir = os.path.join(base_export_dir, cls)\n",
    "\n",
    "    dataset.export(\n",
    "        export_dir=export_dir,\n",
    "        dataset_type=fot.VOCDetectionDataset,\n",
    "        label_field=\"detections\",\n",
    "        export_media=\"move\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Rename folders to meet your file structure\n",
    "    jpeg_images_dir = os.path.join(export_dir, \"data\")\n",
    "    annotations_dir = os.path.join(export_dir, \"labels\")\n",
    "\n",
    "    new_images_dir = os.path.join(export_dir, \"images\")\n",
    "    if os.path.exists(jpeg_images_dir):\n",
    "        if os.path.exists(new_images_dir):\n",
    "            shutil.rmtree(new_images_dir)\n",
    "        os.rename(jpeg_images_dir, new_images_dir)\n",
    "\n",
    "    new_annotations_dir = os.path.join(export_dir, \"annotations\")\n",
    "    if os.path.exists(annotations_dir):\n",
    "        if os.path.exists(new_annotations_dir):\n",
    "            shutil.rmtree(new_annotations_dir)\n",
    "        os.rename(annotations_dir, new_annotations_dir)\n",
    "\n",
    "    print(f\"Exported and organized dataset for '{cls}' at {export_dir}\")\n",
    "\n",
    "    # Delete the dataset and free memory\n",
    "    dataset.delete()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Dataset processing complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 12690488,
     "status": "error",
     "timestamp": 1743980298804,
     "user": {
      "displayName": "Patrick Chao",
      "userId": "07760527590390898325"
     },
     "user_tz": 240
    },
    "id": "WCfhQr4UkRJ1",
    "outputId": "8bb81646-646a-4fc5-cecc-441b9906fc22"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "#########################################\n",
    "# 1. Define the Classes Mapping\n",
    "#########################################\n",
    "# Mapping from string class names to integer labels.\n",
    "# (0 is reserved for background; our classes start at 1.)\n",
    "classes_map = {\n",
    "    \"Dog\": 1,\n",
    "    \"Cat\": 2,\n",
    "    \"Deer\": 3,\n",
    "    \"Bear\": 4,\n",
    "    \"Bird\": 5,\n",
    "    \"Person\": 6,\n",
    "    \"Car\": 7,\n",
    "    \"Truck\": 8,\n",
    "    \"Airplane\": 9,\n",
    "}\n",
    "# Reverse mapping for inference: from integer label to string class name.\n",
    "reverse_classes_map = {v: k for k, v in classes_map.items()}\n",
    "\n",
    "#########################################\n",
    "# 2. Define a Custom Dataset Class to Combine Multiple Datasets\n",
    "#########################################\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for object detection that combines multiple datasets.\n",
    "    Each dataset must follow this structure:\n",
    "\n",
    "        dataset_root/\n",
    "            images/       -> image files\n",
    "            annotations/  -> annotation files (XML, JSON, or text)\n",
    "\n",
    "    Annotation parsing:\n",
    "      - XML: expects an <annotation> element with <object>/<name> and <bndbox>.\n",
    "      - JSON: expects a dict with key \"objects\" holding list of objects.\n",
    "      - Text: each line should be \"xmin ymin xmax ymax label\".\n",
    "\n",
    "    The classes_map converts string class names into integer labels.\n",
    "    Additionally, this class returns meta-information (image and annotation paths)\n",
    "    to help track errors.\n",
    "    \"\"\"\n",
    "    def __init__(self, roots, transforms=None, classes_map=None):\n",
    "        # Ensure roots is a list\n",
    "        if isinstance(roots, str):\n",
    "            roots = [roots]\n",
    "        self.samples = []  # list of tuples: (img_path, ann_path)\n",
    "        self.transforms = transforms\n",
    "        self.classes_map = classes_map\n",
    "\n",
    "        # Gather image/annotation pairs from each dataset root.\n",
    "        for root in roots:\n",
    "            images_dir = os.path.join(root, \"images\")\n",
    "            annotations_dir = os.path.join(root, \"annotations\")\n",
    "            if not os.path.isdir(images_dir) or not os.path.isdir(annotations_dir):\n",
    "                print(f\"Warning: Missing 'images' or 'annotations' in {root}. Skipping this root.\")\n",
    "                continue\n",
    "            imgs = sorted(os.listdir(images_dir))\n",
    "            anns = sorted(os.listdir(annotations_dir))\n",
    "            for img_file, ann_file in zip(imgs, anns):\n",
    "                img_path = os.path.join(images_dir, img_file)\n",
    "                ann_path = os.path.join(annotations_dir, ann_file)\n",
    "                self.samples.append((img_path, ann_path))\n",
    "        print(f\"Total samples loaded: {len(self.samples)}\")\n",
    "\n",
    "    def parse_annotation(self, ann_path):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        ext = os.path.splitext(ann_path)[1].lower()\n",
    "        if ext == '.xml':\n",
    "            tree = ET.parse(ann_path)\n",
    "            root_elem = tree.getroot()\n",
    "            for obj in root_elem.findall('object'):\n",
    "                name = obj.find('name').text\n",
    "                bndbox = obj.find('bndbox')\n",
    "                xmin = float(bndbox.find('xmin').text)\n",
    "                ymin = float(bndbox.find('ymin').text)\n",
    "                xmax = float(bndbox.find('xmax').text)\n",
    "                ymax = float(bndbox.find('ymax').text)\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                if self.classes_map and name in self.classes_map:\n",
    "                    label = self.classes_map[name]\n",
    "                else:\n",
    "                    try:\n",
    "                        label = int(name)\n",
    "                    except:\n",
    "                        label = 0\n",
    "                labels.append(label)\n",
    "        elif ext == '.json':\n",
    "            with open(ann_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            for obj in data.get(\"objects\", []):\n",
    "                name = obj.get(\"name\", \"\")\n",
    "                bndbox = obj.get(\"bndbox\", {})\n",
    "                xmin = float(bndbox.get(\"xmin\", 0))\n",
    "                ymin = float(bndbox.get(\"ymin\", 0))\n",
    "                xmax = float(bndbox.get(\"xmax\", 0))\n",
    "                ymax = float(bndbox.get(\"ymax\", 0))\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                if self.classes_map and name in self.classes_map:\n",
    "                    label = self.classes_map[name]\n",
    "                else:\n",
    "                    try:\n",
    "                        label = int(name)\n",
    "                    except:\n",
    "                        label = 0\n",
    "                labels.append(label)\n",
    "        else:\n",
    "            with open(ann_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        continue\n",
    "                    xmin, ymin, xmax, ymax = map(float, parts[:4])\n",
    "                    label = int(parts[4])\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(label)\n",
    "        return boxes, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, ann_path = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        boxes, labels = self.parse_annotation(ann_path)\n",
    "\n",
    "        # Convert lists to torch tensors.\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        # Return meta info along with image and target\n",
    "        meta = {\"img_path\": img_path, \"ann_path\": ann_path}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target, meta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "#########################################\n",
    "# 3. Define Data Transforms\n",
    "#########################################\n",
    "def get_transform(train):\n",
    "    transforms = [T.ToTensor()]\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "#########################################\n",
    "# 4. Prepare the Combined Dataset and DataLoader\n",
    "#########################################\n",
    "# Define dataset roots (one per class). (Assuming on Colab under /content/dataset)\n",
    "dataset_roots = [\n",
    "    \"/content/dataset/Dog\",\n",
    "    \"/content/dataset/Cat\",\n",
    "    \"/content/dataset/Deer\",\n",
    "    \"/content/dataset/Bear\",\n",
    "    \"/content/dataset/Bird\",\n",
    "    \"/content/dataset/Person\",\n",
    "    \"/content/dataset/Car\",\n",
    "    \"/content/dataset/Truck\",\n",
    "    \"/content/dataset/Airplane\",\n",
    "]\n",
    "\n",
    "# Create the combined dataset.\n",
    "dataset = CustomDataset(dataset_roots, transforms=get_transform(train=True), classes_map=classes_map)\n",
    "\n",
    "# Update the collate function to handle the extra meta info.\n",
    "def collate_fn(batch):\n",
    "    images, targets, metas = zip(*batch)\n",
    "    return list(images), list(targets), list(metas)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "#########################################\n",
    "# 5. Build the Object Detection Model\n",
    "#########################################\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "num_classes = 1 + len(classes_map)  # background + defined classes.\n",
    "model = get_model(num_classes)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "#########################################\n",
    "# 6. Define the Optimizer and Learning Rate Scheduler\n",
    "#########################################\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "#########################################\n",
    "# 7. Training Loop with Exception Handling\n",
    "#########################################\n",
    "error_log = []  # To store errors with corresponding file paths.\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    iteration = 0\n",
    "    for images, targets, metas in data_loader:\n",
    "        # Attempt to process the entire batch.\n",
    "        try:\n",
    "            images_device = [img.to(device) for img in images]\n",
    "            targets_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images_device, targets_device)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "        except Exception as e:\n",
    "            # If batch-level processing fails, try processing each sample individually.\n",
    "            valid_images = []\n",
    "            valid_targets = []\n",
    "            for i in range(len(images)):\n",
    "                try:\n",
    "                    img_i = images[i].to(device)\n",
    "                    target_i = {k: v.to(device) for k, v in targets[i].items()}\n",
    "                    # Process single sample.\n",
    "                    _ = model([img_i], [target_i])\n",
    "                    valid_images.append(images[i])\n",
    "                    valid_targets.append(targets[i])\n",
    "                except Exception as ex:\n",
    "                    error_info = {\n",
    "                        \"img_path\": metas[i][\"img_path\"],\n",
    "                        \"ann_path\": metas[i][\"ann_path\"],\n",
    "                        \"error\": str(ex)\n",
    "                    }\n",
    "                    error_log.append(error_info)\n",
    "                    print(f\"Error processing sample: {error_info}\")\n",
    "            if len(valid_images) == 0:\n",
    "                print(\"Skipping entire batch due to errors.\")\n",
    "                continue\n",
    "            images_device = [img.to(device) for img in valid_images]\n",
    "            targets_device = [{k: v.to(device) for k, v in t.items()} for t in valid_targets]\n",
    "            loss_dict = model(images_device, targets_device)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Iteration {iteration}, Loss: {loss.item():.4f}\")\n",
    "        iteration += 1\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch} completed.\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(f\"Total error samples encountered: {len(error_log)}\")\n",
    "if error_log:\n",
    "    print(\"Error details:\")\n",
    "    for err in error_log:\n",
    "        print(err)\n",
    "\n",
    "# Save the trained model weights.\n",
    "torch.save(model.state_dict(), \"fasterrcnn_model.pth\")\n",
    "\n",
    "#########################################\n",
    "# 8. Inference: Real-Time Object Detection Demo\n",
    "#########################################\n",
    "def predict_and_plot(image_path, model, device, threshold=0.5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img_tensor = transform(img).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img_tensor])\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Iterate over detected bounding boxes.\n",
    "    for i, box in enumerate(prediction[0]['boxes']):\n",
    "        score = prediction[0]['scores'][i].item()\n",
    "        if score > threshold:\n",
    "            # Move the tensor to CPU and convert to numpy\n",
    "            box_np = box.cpu().numpy()\n",
    "            xmin, ymin, xmax, ymax = box_np\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                     linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            label_int = prediction[0]['labels'][i].item()\n",
    "            label_str = reverse_classes_map.get(label_int, \"N/A\")\n",
    "            ax.text(xmin, ymin, f\"{label_str} {score:.2f}\", color='yellow', fontsize=12)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage: Run inference on random sample images.\n",
    "# Assuming your combined dataset is in the variable 'dataset' (an instance of CustomDataset)\n",
    "# and that dataset.samples is a list of (img_path, ann_path) tuples.\n",
    "all_img_paths = [sample[0] for sample in dataset.samples]\n",
    "\n",
    "# Randomly select 10 images\n",
    "random_img_paths = random.sample(all_img_paths, 10)\n",
    "\n",
    "# Run inference on each selected image\n",
    "for img_path in random_img_paths:\n",
    "    print(f\"Processing: {img_path}\")\n",
    "    predict_and_plot(img_path, model, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1tL6Q3kdsX7XUnpgbl1nbLQvmlOEIZ3cc"
    },
    "executionInfo": {
     "elapsed": 5314,
     "status": "ok",
     "timestamp": 1743983267275,
     "user": {
      "displayName": "Patrick Chao",
      "userId": "07760527590390898325"
     },
     "user_tz": 240
    },
    "id": "_ypHICMmdaOQ",
    "outputId": "0878a536-52d3-4a58-e57e-7dcb31eefd19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to perform inference and plot a single image.\n",
    "def predict_and_plot(image_path, model, device, threshold=0.5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img_tensor = transform(img).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img_tensor])\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Iterate over detected bounding boxes.\n",
    "    for i, box in enumerate(prediction[0]['boxes']):\n",
    "        score = prediction[0]['scores'][i].item()\n",
    "        if score > threshold:\n",
    "            # Move the tensor to CPU and convert to numpy\n",
    "            box_np = box.cpu().numpy()\n",
    "            xmin, ymin, xmax, ymax = box_np\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                     linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            label_int = prediction[0]['labels'][i].item()\n",
    "            label_str = reverse_classes_map.get(label_int, \"N/A\")\n",
    "            ax.text(xmin, ymin, f\"{label_str} {score:.2f}\", color='yellow', fontsize=12)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your combined dataset is in the variable 'dataset' (an instance of CustomDataset)\n",
    "# and that dataset.samples is a list of (img_path, ann_path) tuples.\n",
    "all_img_paths = [sample[0] for sample in dataset.samples]\n",
    "\n",
    "# Randomly select 10 images\n",
    "random_img_paths = random.sample(all_img_paths, 10)\n",
    "\n",
    "# Run inference on each selected image\n",
    "for img_path in random_img_paths:\n",
    "    print(f\"Processing: {img_path}\")\n",
    "    predict_and_plot(img_path, model, device)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNXSyfi23iBuDZ8LaX3iZFF",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
